fastapi>=0.104.0
uvicorn>=0.24.0
pydantic>=2.0.0
groq>=0.4.0
requests>=2.31.0
python-multipart>=0.0.6
transformers==4.36.2
datasets==2.14.5
peft==0.7.1
accelerate==0.25.0
trl==0.7.10
torch==2.3.0+cu118
torchvision==0.16.0+cu118
torchaudio==2.3.0+cu118
flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.5.8/flash_attn-2.5.8+cu118torch2.3cxx11abiFALSE-cp311-cp311-linux_x86_64.whl

